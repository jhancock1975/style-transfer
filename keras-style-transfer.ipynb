{"cells":[{"metadata":{"_uuid":"adf9d56e8641afe6f0952f53d54557c5faf09c94"},"cell_type":"markdown","source":"I explore style transfer in this notebook using deep learning. This is one of my favorite applications of deep learning - this was interesting at first because the output is so visual and then even more after learning the technique.\n\nThe goal is to take two images, one with a distinct \"style\" and another with the desired \"content\" and combine so the style of the first image is transferred to the latter.\nThe way this approach of style transfer works is just like most deep learning approaches - specify a loss function and use a neural network to reduce this loss function. In this case, the loss comprises two major parts, \n1. Style loss - by minimizing this, the neural net learns to get closer to the style. \n2. Content loss - this loss ensures the neural net learns not to lose a lot of content. \n\nUsing the content image as the starting point, the neural network slowly starts to reduce the combination of the above losses to generate some fascinating outputs.\n\nTo define these losses, the intermediate layers of a CNN are used. The first layers of a trained model learn basic patterns like lines and curves and as we go deeper, the layers learn more complex patterns like squares and then even more like faces etc. Hence the output of these layers are used to calculate the loss functions. More information about all of this can be found in the resources below.\n\nMost of the code is from [this notebook](https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb). I first read [this blog post](https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398) and then read [the original paper](https://arxiv.org/abs/1508.06576). I haven't done the [FastAI course yet](https://course.fast.ai/) but [this lesson](https://course.fast.ai/lessons/lesson13.html) has some relevant material."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# https://github.com/keras-team/keras/blob/master/examples/neural_style_transfer.py\nfrom __future__ import print_function\nfrom keras.preprocessing.image import load_img, save_img, img_to_array\nimport numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nimport time\nimport tensorflow as tf\nfrom tensorflow.losses import *\n\nfrom keras.applications import vgg19\nfrom keras import backend as K\nfrom glob import glob","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46b972899135e55daf3dbb6a38f2397d0af25fa2"},"cell_type":"markdown","source":"## Getting some images"},{"metadata":{"trusted":true,"_uuid":"394d98043b24e0fbf10363efb61217cf1756eefd"},"cell_type":"code","source":"import urllib.request\nimport os\n\ndirectory = \"images\"\nout_dir = \"output\"\n\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)   \n    \nimg_names = [\"afremo_rain\", \"maldives\", \"miami\", \"mosaic\", \"nyc\", \"nyc2\", \"sf\", \"udnie\", \"wave\"]   \nimg_urls = {\n    f\"{img_name}.jpg\": f\"https://raw.githubusercontent.com/Pradhyo/machine-learning-practice-notebooks/master/style-transfer/images/{img_name}.jpg\" \n    for img_name in img_names\n}    \n        \n# function to download images to directory    \ndef get_images(directory, img_urls):\n    \"\"\"Download images to directory\"\"\"\n    if not os.path.exists(directory):\n        os.makedirs(directory)    \n    for name, url in img_urls.items():\n        urllib.request.urlretrieve(url, directory + \"/\" + name)\n\nget_images(directory, img_urls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"395fca687e3ff0f7ce8f76fad2a4ff11d83a5116"},"cell_type":"code","source":"from os import listdir\n\n# just making sure the images were downloaded successfully\nprint(listdir(directory))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7208f39e1500039163a98023e34f7eadaf36e33"},"cell_type":"code","source":"from IPython.display import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\n\n# function to display list of images\ndef display_images(image_paths):\n    plt.figure(figsize=(20,20))\n    columns = 3\n    for i, image in enumerate(image_paths):\n        plt.subplot(len(image_paths) / columns + 1, columns, i + 1)\n        plt.imshow(mpimg.imread(image))    \n        \ndisplay_images(list(map(lambda img: directory + \"/\" + img, img_urls.keys())))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346c83203fe8f845af155793145293d03f476e4e"},"cell_type":"code","source":"# util function to open, resize and format pictures into appropriate tensors\ndef preprocess_image(image_path, img_nrows, img_ncols):\n    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fcae5dcd894a85af858dcac7a8b12a1f4d57045"},"cell_type":"code","source":"# util function to convert a tensor into a valid image\ndef deprocess_image(x, img_nrows, img_ncols):\n    if K.image_data_format() == 'channels_first':\n        x = x.reshape((3, img_nrows, img_ncols))\n        x = x.transpose((1, 2, 0))\n    else:\n        x = x.reshape((img_nrows, img_ncols, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb3fbec20224a47869fe030d827734790a4a8147"},"cell_type":"markdown","source":"## Loss functions\n\nThe [VGG19](https://keras.io/applications/#vgg19) model is used to calculate the loss functions.\n\n### Style loss\nThe gram matrix of activations of multiple layers of the CNN captures the correlation within these layers while losing the spatial information. This gram matrix for a set of low/high layers for both images is used to calculate the style loss.\n"},{"metadata":{"trusted":true,"_uuid":"7a36184e847f23f427f829a4b3ecab93e9f26077"},"cell_type":"code","source":"# compute the neural style loss\n# first we need to define 4 util functions\n\n# the gram matrix of an image tensor (feature-wise outer product)\ndef gram_matrix(x):\n    assert K.ndim(x) == 3\n    if K.image_data_format() == 'channels_first':\n        features = K.batch_flatten(x)\n    else:\n        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16b8bf9a38787c8a688d96ddc8bb5240715ac8f9"},"cell_type":"code","source":"# the \"style loss\" is designed to maintain\n# the style of the reference image in the generated image.\n# It is based on the gram matrices (which capture style) of\n# feature maps from the style reference image\n# and from the generated image\ndef style_loss(style, combination):\n    assert K.ndim(style) == 3\n    assert K.ndim(combination) == 3\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    return absolute_difference(C, S) ** 2\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa59202ae1bb3d0446798c56542a8e219cd84b1f"},"cell_type":"markdown","source":"### Content loss\nThe content loss is calculated using the activation of one of the later layers of the CNN which captures more complex patterns."},{"metadata":{"trusted":true,"_uuid":"4f4fc72541a6d4a1d0ee97e7a9b9da086b729c63"},"cell_type":"code","source":"# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\ndef content_loss(base, combination):\n    _content_loss = K.sum(K.square(combination - base))\n    return _content_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8766caab2ac9f1a9cb76915997e4d4c8039683b0"},"cell_type":"markdown","source":"### Variation Loss\nThis loss ensures the images are too pixelated by using the difference between adjacent pixels in the loss function.\n"},{"metadata":{"trusted":true,"_uuid":"17e587b42003764c3374c68809955c85c0f142e8"},"cell_type":"code","source":"# the 3rd loss function, total variation loss,\n# designed to keep the generated image locally coherent\ndef total_variation_loss(x):\n    img_nrows = x.get_shape()[1]\n    img_ncols = x.get_shape()[2]\n    assert K.ndim(x) == 4\n    if K.image_data_format() == 'channels_first':\n        a = K.square(\n            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n        b = K.square(\n            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n    else:\n        a = K.square(\n            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n        b = K.square(\n            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n    _total_variation_loss = K.sum(K.pow(a + b, 1.25))\n    return(_total_variation_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"887ac762b5eb2aa6e0c900ca145a56b9164f8455"},"cell_type":"code","source":"# this Evaluator class makes it possible\n# to compute loss and gradients in one pass\n# while retrieving them via two separate functions,\n# \"loss\" and \"grads\". This is done because scipy.optimize\n# requires separate functions for loss and gradients,\n# but computing them separately would be inefficient.\nclass Evaluator(object):\n\n    def __init__(self, f_outputs, img_nrows, img_ncols):\n        self.loss_value = None\n        self.grads_values = None\n        self.f_outputs = f_outputs\n        self.img_nrows = img_nrows\n        self.img_ncols = img_ncols\n        \n    def eval_loss_and_grads(self, x):\n        if K.image_data_format() == 'channels_first':\n            x = x.reshape((1, 3, self.img_nrows, self.img_ncols))\n        else:\n            x = x.reshape((1, self.img_nrows, self.img_ncols, 3))\n        outs = self.f_outputs([x])\n        loss_value = outs[0]\n        if len(outs[1:]) == 1:\n            grad_values = outs[1].flatten().astype('float64')\n        else:\n            grad_values = np.array(outs[1:]).flatten().astype('float64')\n        return loss_value, grad_values        \n\n    def loss(self, x):\n        assert self.loss_value is None\n        loss_value, grad_values = self.eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3932442c0255eb1a2a3a6532d62b8141276dd52"},"cell_type":"markdown","source":"I modified the code a little bit to make it easy to plug in any loss function for easier experimentation."},{"metadata":{"trusted":true,"_uuid":"6028db1c45da95fe842d32507cbcb7e53a6e9fe7"},"cell_type":"code","source":"def loss1(base_image, style_reference_image, combination_image):\n    content_weight = 1.0\n    style_weight = 1.0\n    total_variation_weight = 1.0\n    \n    # combine the 3 images into a single Keras tensor\n    input_tensor = K.concatenate([base_image,\n                                  style_reference_image,\n                                  combination_image], axis=0)\n    \n    # build the VGG19 network with our 3 images as input\n    # the model will be loaded with pre-trained ImageNet weights\n    model = vgg19.VGG19(input_tensor=input_tensor,\n                        weights='imagenet', include_top=False)\n    print('Model loaded.')\n    \n    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n\n\n    # combine these loss functions into a single scalar\n    loss = K.variable(0.0)\n    layer_features = outputs_dict['block5_conv2']\n    base_image_features = layer_features[0, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    loss += content_weight * content_loss(base_image_features,\n                                          combination_features)\n\n    feature_layers = [f'block{i}_conv{j}' for i in range(4, 6) for j in range(1, 5)]\n    for layer_name in feature_layers:\n        layer_features = outputs_dict[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        sl = style_loss(style_reference_features, combination_features)\n        loss += (style_weight / len(feature_layers)) * sl\n    loss += total_variation_weight * total_variation_loss(combination_image)\n    return loss\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c34be01c0f0bf5e0d2df15c8d87c0f411b03820"},"cell_type":"markdown","source":"Here's the function that takes the two images and a loss function to perform the style transfer."},{"metadata":{"trusted":true,"_uuid":"065655f592f27ae145281c77709f51f1f35e1a32"},"cell_type":"code","source":"def style_transfer(base_image_path, \n                   style_reference_image_path, \n                   result_prefix, \n                   loss_fn,\n                   iterations=10):\n\n    # Remove existing files with prefix\n    for existing_result_file in glob(f\"{result_prefix}*\"):\n        os.remove(existing_result_file)    \n    \n    # dimensions of the generated picture.\n    width, height = load_img(base_image_path).size\n    img_nrows = 400\n    img_ncols = int(width * img_nrows / height)\n\n    # get tensor representations of our images\n    base_image = K.variable(preprocess_image(base_image_path, img_nrows, img_ncols))\n    style_reference_image = K.variable(preprocess_image(style_reference_image_path, img_nrows, img_ncols))\n\n    # this will contain our generated image\n    if K.image_data_format() == 'channels_first':\n        combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n    else:\n        combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n\n\n    loss = loss_fn(base_image, style_reference_image, combination_image)\n    \n    # get the gradients of the generated image wrt the loss\n    grads = K.gradients(loss, combination_image)\n\n    outputs = [loss]\n    if isinstance(grads, (list, tuple)):\n        outputs += grads\n    else:\n        outputs.append(grads)\n\n    f_outputs = K.function([combination_image], outputs)\n\n    evaluator = Evaluator(f_outputs, img_nrows, img_ncols)\n\n    # run scipy-based optimization (L-BFGS) over the pixels of the generated image\n    # so as to minimize the neural style loss\n    x = preprocess_image(base_image_path, img_nrows, img_ncols)\n\n    curr_loss = 0.0\n    for i in range(iterations):\n        x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                         fprime=evaluator.grads, maxfun=20)\n        print('Iteration ' + str(i) + ' loss value:', min_val)\n        # save current generated image\n        img = deprocess_image(x.copy(), img_nrows, img_ncols)\n        # save every 10 images and the last one    \n        if (i % 10 == 0) or (i == iterations - 1):\n            fname = result_prefix + '_at_iteration_%d.png' % i\n            save_img(fname, img)\n\n        # Stop if loss doesn't reduce by more than 10%    \n        if curr_loss and (((curr_loss - min_val)/curr_loss) <= 0.03):\n            fname = result_prefix + '_at_iteration_%d.png' % i\n            save_img(fname, img)            \n            break   \n        \n        curr_loss = min_val            ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d204a82017ef63accf56e1f58ca3552630538c6b"},"cell_type":"markdown","source":"Here I am finally running the functions above to see if everything is working as expected. "},{"metadata":{"trusted":true,"_uuid":"010a3789d2dd1f0bdf58132fc027e1ae3693ab14"},"cell_type":"code","source":"# Running an image with a style\niterations = 5\ninput_name = \"nyc\"\nstyle = \"wave\"\n\nresult_prefix = \"output/\" + input_name + \"_\" + style\nprint(\"Input: \" + input_name + \"; Style: \" + style)\nstyle_transfer(\"images/\" + input_name + \".jpg\", \n               \"images/\" + style + \".jpg\", \n               result_prefix, \n               loss1,\n               iterations)\n\ndisplay_images(sorted(glob(f\"{result_prefix}*\")))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09d9ac66bf4843bcce777c8ae7e449210911d409"},"cell_type":"markdown","source":"Looks like there is no issue losing content but the neural net is not able to capture style as much as I want it to. Below is an identical loss function from before with more weightage given to the style.\n"},{"metadata":{"trusted":true,"_uuid":"6ba16b68f0830781ffab58774a303525f3352a2f"},"cell_type":"code","source":"def loss2(base_image, style_reference_image, combination_image):\n    content_weight = 1.0\n    style_weight = 3.0\n    total_variation_weight = 1.0\n    \n    # combine the 3 images into a single Keras tensor\n    input_tensor = K.concatenate([base_image,\n                                  style_reference_image,\n                                  combination_image], axis=0)\n    \n    # build the VGG19 network with our 3 images as input\n    # the model will be loaded with pre-trained ImageNet weights\n    model = vgg19.VGG19(input_tensor=input_tensor,\n                        weights='imagenet', include_top=False)\n    print('Model loaded.')\n    \n    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n\n\n    # combine these loss functions into a single scalar\n    loss = K.variable(0.0)\n    layer_features = outputs_dict['block5_conv2']\n    base_image_features = layer_features[0, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    loss += content_weight * content_loss(base_image_features,\n                                          combination_features)\n\n    # https://www.kaggle.com/pradhyo/keras-style-transfer-different-losses\n    feature_layers = [f'block{i}_conv{j}' for i in range(4, 6) for j in range(1, 5)]\n    \n    for layer_name in feature_layers:\n        layer_features = outputs_dict[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        sl = style_loss(style_reference_features, combination_features)\n        loss += (style_weight / len(feature_layers)) * sl\n    loss += total_variation_weight * total_variation_loss(combination_image)  \n    return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ed6d72ccb174ddc28f991b8c7f4c2933fd51bd"},"cell_type":"code","source":"# Running an image with a style\niterations = 10\ninput_name = \"nyc\"\nstyle = \"wave\"\n\nresult_prefix = \"output/\" + input_name + \"_\" + style\nprint(\"Input: \" + input_name + \"; Style: \" + style)\nstyle_transfer(\"images/\" + input_name + \".jpg\", \n               \"images/\" + style + \".jpg\", \n               result_prefix, \n               loss2,\n               iterations)\n\ndisplay_images(sorted(glob(f\"{result_prefix}*\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6d63ad7c2b37628786a9fff137b6eae93636b76"},"cell_type":"code","source":"# Running multiple images with multiple styles\niterations = 50\n\ninput_names = ['sf', 'nyc2', 'miami', 'maldives', 'nyc']\nstyles = ['mosaic', 'udnie', 'wave', 'afremo_rain']\nfor input_name in input_names:\n    for style in styles:\n        result_prefix = \"output/\" + input_name + \"_\" + style\n        print(\"Input: \" + input_name + \"; Style: \" + style)\n        style_transfer(f\"images/{input_name}.jpg\", \n                       f\"images/{style}.jpg\", \n                       result_prefix, \n                       loss2,\n                       iterations)\n\ndisplay_images(sorted(glob(f\"{result_prefix}*\")))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd4ecaac4c12ad885aa9beb8c44c06928716ef61"},"cell_type":"markdown","source":"## Reflections\n\n- Based on my experiments, it is best to pick \"content\" images that will still look good despite some loss in detail and pick \"style\" images with distince colors/texture/style.\n- Currently the neural network learns each style image each time - this can be improved by teaching a convnet a specific style over multiple cycles and then using that model to quickly copy over the style to fresh content images.\n- The loss function is a clear place to tweak to try and produce better output images. The [Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric) seems to produce better results based on [this](https://github.com/VinceMarron/style_transfer).\n- Using other layers in the loss functions should also produce different results\n\n## Resources\n1. [Neural Style Transfer](https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/8.3-neural-style-transfer.ipynb) (notebook)\n2. [Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution](https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398) (blog post)\n3. [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576) (paper)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}